{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils.print as print_f\n",
    "\n",
    "from utils.engine import xami_evaluate, get_iou_types\n",
    "from utils.plot import plot_losses, plot_ap_ars\n",
    "\n",
    "from models.setup import ModelSetup\n",
    "from models.build import create_multimodal_rcnn_model\n",
    "from models.train import TrainingInfo\n",
    "from utils.save import check_best, end_train, get_data_from_metric_logger\n",
    "from data.load import get_datasets, get_dataloaders\n",
    "from IPython.display import clear_output\n",
    "from utils.eval import get_ap_ar, get_ap_ar_for_train_val\n",
    "from utils.train import get_optimiser, get_lr_scheduler, print_params_setup, get_coco_eval_params, get_dynamic_loss, get_params\n",
    "from utils.init import reproducibility, clean_memory_get_device\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS\n",
    "from data.paths import MIMIC_EYE_PATH\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "## Suppress the assignement warning from pandas.r\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "## Supress user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CPU]\n"
     ]
    }
   ],
   "source": [
    "device = clean_memory_get_device()\n",
    "reproducibility()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.transforms import get_tensorise_h_flip_transform\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS\n",
    "from data.paths import MIMIC_EYE_PATH\n",
    "from data.datasets import ReflacxObjectDetectionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data.datasets import collate_fn\n",
    "from data.load import seed_worker, get_dataloader_g\n",
    "\n",
    "dataset_params_dict = {\n",
    "    \"MIMIC_EYE_PATH\": MIMIC_EYE_PATH,\n",
    "    # \"with_clinical\": model_setup.use_clinical,\n",
    "    \"bbox_to_mask\": True,\n",
    "    \"labels_cols\": DEFAULT_REFLACX_LABEL_COLS,\n",
    "}\n",
    "\n",
    "train_dataset = ReflacxObjectDetectionDataset(\n",
    "        **dataset_params_dict, split_str=\"train\", transforms=get_tensorise_h_flip_transform(train=False), \n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=get_dataloader_g(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.components.feature_extractors import ImageFeatureExtractor\n",
    "from models.components.fusors import NoActionFusor\n",
    "from models.components.task_performers import ObjectDetectionWithMaskParameters, ObjectDetectionWithMaskPerformer\n",
    "from models.frameworks import ExtractFusePerform\n",
    "from models.backbones import get_normal_backbone\n",
    "from models.setup import ModelSetup\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CPU]\n"
     ]
    }
   ],
   "source": [
    "from utils.init import reproducibility, clean_memory_get_device\n",
    "\n",
    "device = clean_memory_get_device()\n",
    "reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained backbone. mobilenet_v3\n"
     ]
    }
   ],
   "source": [
    "setup = ModelSetup()\n",
    "backbone = get_normal_backbone(setup)\n",
    "image_extractor = ImageFeatureExtractor(backbone)\n",
    "fusor = NoActionFusor()\n",
    "params = ObjectDetectionWithMaskParameters()\n",
    "performer = ObjectDetectionWithMaskPerformer(\n",
    "    params,\n",
    "    image_extractor.backbone.out_channels,\n",
    "    len(DEFAULT_REFLACX_LABEL_COLS) + 1\n",
    ")\n",
    "# get the backbone\n",
    "model = ExtractFusePerform(\n",
    "    feature_extractors={\"image\": image_extractor},\n",
    "    fusor=fusor,\n",
    "    task_performers={\"object-detection\": performer},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained backbone. mobilenet_v3\n"
     ]
    }
   ],
   "source": [
    "setup = ModelSetup()\n",
    "backbone = get_normal_backbone(setup)\n",
    "image_extractor = ImageFeatureExtractor(backbone)\n",
    "fusor = NoActionFusor()\n",
    "params = ObjectDetectionWithMaskParameters()\n",
    "performer = ObjectDetectionWithMaskPerformer(\n",
    "    params,\n",
    "    image_extractor.backbone.out_channels,\n",
    "    len(DEFAULT_REFLACX_LABEL_COLS) + 1\n",
    ")\n",
    "# get the backbone\n",
    "model = ExtractFusePerform(\n",
    "    feature_extractors={\"image\": image_extractor},\n",
    "    fusor=fusor,\n",
    "    task_performers={\"object-detection\": performer},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data = train_dataset.prepare_input_from_data(data, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Preparing for the training.====================\n"
     ]
    }
   ],
   "source": [
    "print_f.print_title(\"Preparing for the training.\")\n",
    "train_info = TrainingInfo(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractFusePerform(\n",
       "  (fusor): NoActionFusor()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from data.load import get_datasets, get_dataloaders\n",
    "from utils.coco_utils import get_cocos\n",
    "from utils.coco_eval import get_eval_params_dict\n",
    "\n",
    "\n",
    "detect_eval_dataset, train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "        dataset_params_dict=dataset_params_dict,\n",
    "    )\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size=setup.batch_size,\n",
    ")\n",
    "\n",
    "train_coco, val_coco, test_coco = get_cocos(\n",
    "    train_dataloader, val_dataloader, test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "iou_thrs = np.array([0.5])\n",
    "use_iobb = True\n",
    "\n",
    "eval_params_dict = get_eval_params_dict(\n",
    "    detect_eval_dataset, iou_thrs=iou_thrs, use_iobb=use_iobb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicWeightedLoss(\n",
       "  (params): ParameterDict(\n",
       "      (loss_mask): Parameter containing: [torch.FloatTensor of size 1]\n",
       "      (object-detection_loss_box_reg): Parameter containing: [torch.FloatTensor of size 1]\n",
       "      (object-detection_loss_classifier): Parameter containing: [torch.FloatTensor of size 1]\n",
       "      (object-detection_loss_mask): Parameter containing: [torch.FloatTensor of size 1]\n",
       "      (object-detection_loss_objectness): Parameter containing: [torch.FloatTensor of size 1]\n",
       "      (object-detection_loss_rpn_box_reg): Parameter containing: [torch.FloatTensor of size 1]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.dynamic_loss import DynamicWeightedLoss\n",
    "\n",
    "# loss_keys = [\n",
    "#     \"loss_classifier\",\n",
    "#     \"loss_box_reg\",\n",
    "#     \"loss_objectness\",\n",
    "#     \"loss_rpn_box_reg\"\n",
    "# ]\n",
    "\n",
    "loss_keys = [\n",
    "    \"object-detection_loss_box_reg\",\n",
    "    \"object-detection_loss_classifier\",\n",
    "    \"object-detection_loss_mask\",\n",
    "    \"object-detection_loss_objectness\",\n",
    "    \"object-detection_loss_rpn_box_reg\",\n",
    "]\n",
    "\n",
    "dynamic_loss_weight = DynamicWeightedLoss(\n",
    "    keys=loss_keys + [\"loss_mask\"] if setup.use_mask else loss_keys\n",
    ")\n",
    "dynamic_loss_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model]: 0\n",
      "Using SGD as optimizer with lr=0.0005\n",
      "====================Start training. Preparing Took [455] sec====================\n"
     ]
    }
   ],
   "source": [
    "print_params_setup(model)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "if dynamic_loss_weight:\n",
    "    params += [p for p in dynamic_loss_weight.parameters() if p.requires_grad]\n",
    "\n",
    "iou_types = get_iou_types(model, setup)\n",
    "optimizer = get_optimiser(params, setup)\n",
    "lr_scheduler = get_lr_scheduler(optimizer, setup)\n",
    "\n",
    "current_time = datetime.now()\n",
    "\n",
    "print_f.print_title(\n",
    "    f\"Start training. Preparing Took [{ (current_time - train_info.start_t).seconds}] sec\"\n",
    ")\n",
    "\n",
    "train_info.start_t = datetime.now()\n",
    "\n",
    "val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "\n",
    "\n",
    "# outputs = model({\"image\": data[0]}, targets={\n",
    "#                            \"object-detection\": data[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all loses values from it.\n",
    "\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.datasets.ReflacxObjectDetectionDataset at 0x1201edba0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/531]  eta: 1:27:52  lr: 0.000500  loss: 3.2896 (3.2896)  object-detection_loss_classifier: 1.8488 (1.8488)  object-detection_loss_box_reg: 0.0343 (0.0343)  object-detection_loss_mask: 0.7119 (0.7119)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0018 (0.0018)  time: 9.9292  data: 0.2835\n",
      "Epoch: [0]  [ 10/531]  eta: 1:21:45  lr: 0.000500  loss: 3.1899 (3.1952)  object-detection_loss_classifier: 1.8457 (1.8468)  object-detection_loss_box_reg: 0.0343 (0.0340)  object-detection_loss_mask: 0.6128 (0.6170)  object-detection_loss_objectness: 0.6929 (0.6930)  object-detection_loss_rpn_box_reg: 0.0037 (0.0044)  time: 9.4155  data: 0.3040\n",
      "Epoch: [0]  [ 20/531]  eta: 1:18:22  lr: 0.000500  loss: 3.1474 (3.1945)  object-detection_loss_classifier: 1.8454 (1.8463)  object-detection_loss_box_reg: 0.0222 (0.0258)  object-detection_loss_mask: 0.5936 (0.6255)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0033 (0.0039)  time: 9.1659  data: 0.3018\n",
      "Epoch: [0]  [ 30/531]  eta: 1:17:14  lr: 0.000500  loss: 3.1546 (3.2069)  object-detection_loss_classifier: 1.8463 (1.8464)  object-detection_loss_box_reg: 0.0126 (0.0235)  object-detection_loss_mask: 0.6040 (0.6407)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0025 (0.0033)  time: 9.1588  data: 0.3047\n",
      "Epoch: [0]  [ 40/531]  eta: 1:15:20  lr: 0.000500  loss: 3.1868 (3.2132)  object-detection_loss_classifier: 1.8449 (1.8461)  object-detection_loss_box_reg: 0.0150 (0.0229)  object-detection_loss_mask: 0.6175 (0.6479)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0027 (0.0035)  time: 9.2122  data: 0.3093\n",
      "Epoch: [0]  [ 50/531]  eta: 1:13:29  lr: 0.000500  loss: 3.1609 (3.1870)  object-detection_loss_classifier: 1.8455 (1.8462)  object-detection_loss_box_reg: 0.0150 (0.0220)  object-detection_loss_mask: 0.5985 (0.6224)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0030 (0.0034)  time: 9.0382  data: 0.3050\n",
      "Epoch: [0]  [ 60/531]  eta: 1:10:59  lr: 0.000500  loss: 3.1609 (3.1853)  object-detection_loss_classifier: 1.8462 (1.8462)  object-detection_loss_box_reg: 0.0162 (0.0217)  object-detection_loss_mask: 0.5975 (0.6213)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0018 (0.0033)  time: 8.7060  data: 0.2973\n",
      "Epoch: [0]  [ 70/531]  eta: 1:08:47  lr: 0.000500  loss: 3.1678 (3.1839)  object-detection_loss_classifier: 1.8459 (1.8462)  object-detection_loss_box_reg: 0.0199 (0.0224)  object-detection_loss_mask: 0.5879 (0.6191)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0028 (0.0033)  time: 8.4119  data: 0.2908\n",
      "Epoch: [0]  [ 80/531]  eta: 1:06:43  lr: 0.000500  loss: 3.1678 (3.1868)  object-detection_loss_classifier: 1.8459 (1.8462)  object-detection_loss_box_reg: 0.0199 (0.0216)  object-detection_loss_mask: 0.6054 (0.6228)  object-detection_loss_objectness: 0.6930 (0.6929)  object-detection_loss_rpn_box_reg: 0.0033 (0.0033)  time: 8.3724  data: 0.2933\n",
      "Epoch: [0]  [ 90/531]  eta: 1:04:42  lr: 0.000500  loss: 3.1563 (3.1841)  object-detection_loss_classifier: 1.8463 (1.8463)  object-detection_loss_box_reg: 0.0187 (0.0215)  object-detection_loss_mask: 0.6070 (0.6201)  object-detection_loss_objectness: 0.6930 (0.6929)  object-detection_loss_rpn_box_reg: 0.0026 (0.0033)  time: 8.2724  data: 0.2941\n",
      "Epoch: [0]  [100/531]  eta: 1:02:52  lr: 0.000500  loss: 3.1393 (3.1809)  object-detection_loss_classifier: 1.8465 (1.8463)  object-detection_loss_box_reg: 0.0203 (0.0215)  object-detection_loss_mask: 0.5694 (0.6170)  object-detection_loss_objectness: 0.6928 (0.6929)  object-detection_loss_rpn_box_reg: 0.0024 (0.0033)  time: 8.2445  data: 0.2940\n",
      "Epoch: [0]  [110/531]  eta: 1:01:23  lr: 0.000500  loss: 3.1492 (3.1842)  object-detection_loss_classifier: 1.8455 (1.8462)  object-detection_loss_box_reg: 0.0200 (0.0217)  object-detection_loss_mask: 0.5887 (0.6200)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0034 (0.0033)  time: 8.4974  data: 0.2985\n",
      "Epoch: [0]  [120/531]  eta: 0:59:49  lr: 0.000500  loss: 3.1560 (3.1796)  object-detection_loss_classifier: 1.8451 (1.8462)  object-detection_loss_box_reg: 0.0200 (0.0218)  object-detection_loss_mask: 0.5876 (0.6155)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0034 (0.0033)  time: 8.6411  data: 0.2970\n",
      "Epoch: [0]  [130/531]  eta: 0:58:22  lr: 0.000500  loss: 3.1378 (3.1693)  object-detection_loss_classifier: 1.8454 (1.8462)  object-detection_loss_box_reg: 0.0180 (0.0215)  object-detection_loss_mask: 0.5646 (0.6054)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0037 (0.0033)  time: 8.6495  data: 0.2935\n",
      "Epoch: [0]  [140/531]  eta: 0:56:47  lr: 0.000500  loss: 3.1466 (3.1719)  object-detection_loss_classifier: 1.8454 (1.8461)  object-detection_loss_box_reg: 0.0151 (0.0211)  object-detection_loss_mask: 0.5929 (0.6085)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0032 (0.0033)  time: 8.6030  data: 0.2934\n",
      "Epoch: [0]  [150/531]  eta: 0:55:13  lr: 0.000500  loss: 3.2423 (3.1771)  object-detection_loss_classifier: 1.8451 (1.8461)  object-detection_loss_box_reg: 0.0165 (0.0216)  object-detection_loss_mask: 0.6711 (0.6131)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0032 (0.0033)  time: 8.4513  data: 0.2887\n",
      "Epoch: [0]  [160/531]  eta: 0:53:37  lr: 0.000500  loss: 3.2354 (3.1793)  object-detection_loss_classifier: 1.8453 (1.8460)  object-detection_loss_box_reg: 0.0291 (0.0221)  object-detection_loss_mask: 0.6548 (0.6149)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0038 (0.0034)  time: 8.3757  data: 0.2895\n",
      "Epoch: [0]  [170/531]  eta: 0:52:07  lr: 0.000500  loss: 3.1416 (3.1759)  object-detection_loss_classifier: 1.8449 (1.8460)  object-detection_loss_box_reg: 0.0183 (0.0217)  object-detection_loss_mask: 0.5905 (0.6120)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0028 (0.0034)  time: 8.4041  data: 0.2951\n",
      "Epoch: [0]  [180/531]  eta: 0:50:29  lr: 0.000500  loss: 3.1310 (3.1744)  object-detection_loss_classifier: 1.8463 (1.8460)  object-detection_loss_box_reg: 0.0106 (0.0213)  object-detection_loss_mask: 0.5633 (0.6109)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0023 (0.0033)  time: 8.2810  data: 0.2840\n",
      "Epoch: [0]  [190/531]  eta: 0:49:13  lr: 0.000500  loss: 3.1599 (3.1752)  object-detection_loss_classifier: 1.8468 (1.8461)  object-detection_loss_box_reg: 0.0209 (0.0217)  object-detection_loss_mask: 0.6105 (0.6113)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0019 (0.0033)  time: 8.6570  data: 0.2876\n",
      "Epoch: [0]  [200/531]  eta: 0:47:46  lr: 0.000500  loss: 3.2249 (3.1729)  object-detection_loss_classifier: 1.8462 (1.8460)  object-detection_loss_box_reg: 0.0251 (0.0218)  object-detection_loss_mask: 0.6473 (0.6089)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0039 (0.0034)  time: 8.9243  data: 0.2969\n",
      "Epoch: [0]  [210/531]  eta: 0:46:09  lr: 0.000500  loss: 3.2503 (3.1772)  object-detection_loss_classifier: 1.8450 (1.8460)  object-detection_loss_box_reg: 0.0323 (0.0223)  object-detection_loss_mask: 0.6604 (0.6126)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0045 (0.0034)  time: 8.2943  data: 0.2870\n",
      "Epoch: [0]  [220/531]  eta: 0:44:37  lr: 0.000500  loss: 3.2573 (3.1769)  object-detection_loss_classifier: 1.8456 (1.8460)  object-detection_loss_box_reg: 0.0218 (0.0222)  object-detection_loss_mask: 0.6651 (0.6125)  object-detection_loss_objectness: 0.6928 (0.6929)  object-detection_loss_rpn_box_reg: 0.0028 (0.0034)  time: 8.0985  data: 0.2864\n",
      "Epoch: [0]  [230/531]  eta: 0:43:04  lr: 0.000500  loss: 3.1847 (3.1756)  object-detection_loss_classifier: 1.8462 (1.8460)  object-detection_loss_box_reg: 0.0178 (0.0221)  object-detection_loss_mask: 0.6136 (0.6112)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0029 (0.0034)  time: 8.1689  data: 0.2863\n",
      "Epoch: [0]  [240/531]  eta: 0:41:37  lr: 0.000500  loss: 3.2286 (3.1786)  object-detection_loss_classifier: 1.8462 (1.8460)  object-detection_loss_box_reg: 0.0178 (0.0219)  object-detection_loss_mask: 0.6464 (0.6144)  object-detection_loss_objectness: 0.6929 (0.6929)  object-detection_loss_rpn_box_reg: 0.0035 (0.0034)  time: 8.3118  data: 0.2854\n",
      "Epoch: [0]  [250/531]  eta: 0:40:14  lr: 0.000500  loss: 3.2589 (3.1808)  object-detection_loss_classifier: 1.8467 (1.8461)  object-detection_loss_box_reg: 0.0191 (0.0219)  object-detection_loss_mask: 0.6941 (0.6165)  object-detection_loss_objectness: 0.6928 (0.6929)  object-detection_loss_rpn_box_reg: 0.0034 (0.0034)  time: 8.6372  data: 0.2927\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m train_one_epoch\n\u001b[0;32m----> 3\u001b[0m train_info\u001b[39m.\u001b[39mlast_train_evaluator, train_loger \u001b[39m=\u001b[39m train_one_epoch(\n\u001b[1;32m      4\u001b[0m     setup\u001b[39m=\u001b[39;49msetup,\n\u001b[1;32m      5\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m      7\u001b[0m     data_loader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      8\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m      9\u001b[0m     epoch\u001b[39m=\u001b[39;49mtrain_info\u001b[39m.\u001b[39;49mepoch,\n\u001b[1;32m     10\u001b[0m     print_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     iou_types\u001b[39m=\u001b[39;49miou_types,\n\u001b[1;32m     12\u001b[0m     coco\u001b[39m=\u001b[39;49mtrain_coco,\n\u001b[1;32m     13\u001b[0m     score_thres\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m     evaluate_on_run\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m     params_dict\u001b[39m=\u001b[39;49meval_params_dict,\n\u001b[1;32m     16\u001b[0m     dynamic_loss_weight\u001b[39m=\u001b[39;49mdynamic_loss_weight,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/MIMIC-Eye-applications/utils/engine.py:75\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(setup, model, optimizer, data_loader, device, epoch, print_freq, iou_types, coco, score_thres, evaluate_on_run, params_dict, dynamic_loss_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m inputs, targets \u001b[39m=\u001b[39m data_loader\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mprepare_input_from_data(data, device)\n\u001b[1;32m     74\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 75\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs, targets\u001b[39m=\u001b[39;49mtargets)\n\u001b[1;32m     76\u001b[0m     \u001b[39m# loss_dict = loss_multiplier(loss_dict,epoch)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     all_losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/MIMIC-Eye-applications/models/frameworks.py:26\u001b[0m, in \u001b[0;36mExtractFusePerform.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     22\u001b[0m feature_maps \u001b[39m=\u001b[39m { k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractors[k](x) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractors\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     24\u001b[0m fused \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfusor(feature_maps)\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[39m=\u001b[39m { k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_performers[k](x, fused, targets[k]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_performers\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/GitHub/MIMIC-Eye-applications/models/frameworks.py:26\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m feature_maps \u001b[39m=\u001b[39m { k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractors[k](x) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractors\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     24\u001b[0m fused \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfusor(feature_maps)\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[39m=\u001b[39m { k: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask_performers[k](x, fused, targets[k]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_performers\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/MIMIC-Eye-applications/models/components/task_performers.py:123\u001b[0m, in \u001b[0;36mObjectDetectionWithMaskPerformer.forward\u001b[0;34m(self, x, z, targets)\u001b[0m\n\u001b[1;32m    114\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(x[\u001b[39m'\u001b[39m\u001b[39mimage_list\u001b[39m\u001b[39m'\u001b[39m], z, targets)\n\u001b[1;32m    116\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(\n\u001b[1;32m    117\u001b[0m     z,\n\u001b[1;32m    118\u001b[0m     proposals,\n\u001b[1;32m    119\u001b[0m     x[\u001b[39m\"\u001b[39m\u001b[39mimage_list\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mimage_sizes,\n\u001b[1;32m    120\u001b[0m     targets,\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 123\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpostprocess(\n\u001b[1;32m    124\u001b[0m     detections, x[\u001b[39m\"\u001b[39;49m\u001b[39mimage_list\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mimage_sizes, x[\u001b[39m'\u001b[39;49m\u001b[39moriginal_image_sizes\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m losses \u001b[39m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m losses\u001b[39m.\u001b[39mupdate(detector_losses)\n",
      "File \u001b[0;32m~/Documents/GitHub/MIMIC-Eye-applications/models/components/task_performers.py:153\u001b[0m, in \u001b[0;36mObjectDetectionWithMaskPerformer.postprocess\u001b[0;34m(self, result, image_shapes, original_image_sizes)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m pred:\n\u001b[1;32m    152\u001b[0m     masks \u001b[39m=\u001b[39m pred[\u001b[39m\"\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 153\u001b[0m     masks \u001b[39m=\u001b[39m paste_masks_in_image(masks, boxes, o_im_s)\n\u001b[1;32m    154\u001b[0m     result[i][\u001b[39m\"\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m masks\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mkeypoints\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m pred:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/models/detection/roi_heads.py:486\u001b[0m, in \u001b[0;36mpaste_masks_in_image\u001b[0;34m(masks, boxes, img_shape, padding)\u001b[0m\n\u001b[1;32m    484\u001b[0m res \u001b[39m=\u001b[39m [paste_mask_in_image(m[\u001b[39m0\u001b[39m], b, im_h, im_w) \u001b[39mfor\u001b[39;00m m, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(masks, boxes)]\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(res) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(res, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     ret \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mnew_empty((\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, im_h, im_w))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.engine import train_one_epoch\n",
    "\n",
    "train_info.last_train_evaluator, train_loger = train_one_epoch(\n",
    "    setup=setup,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    device=device,\n",
    "    epoch=train_info.epoch,\n",
    "    print_freq=10,\n",
    "    iou_types=iou_types,\n",
    "    coco=train_coco,\n",
    "    score_thres=None,\n",
    "    evaluate_on_run=True,\n",
    "    params_dict=eval_params_dict,\n",
    "    dynamic_loss_weight=dynamic_loss_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52a48fdedee40b77eb251917c5aa239bf02f1ab8c93cc13fe7347f570eadc6b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
