{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Linear(16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Conv2d(16, 32, kernel_size=1, padding=0, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]]],\n",
       "\n",
       "\n",
       "        [[[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]]],\n",
       "\n",
       "\n",
       "        [[[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]]],\n",
       "\n",
       "\n",
       "        [[[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]]],\n",
       "\n",
       "\n",
       "        [[[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]],\n",
       "\n",
       "         [[-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085],\n",
       "          [-0.0561,  0.9532, -1.3070,  ...,  0.8207,  0.0554,  0.9085]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(torch.ones(5, 16, 4,4).permute(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 4, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(torch.ones(5, 16, 4,4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_tensor = torch.ones(5, 16, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def srf_fusion(images, level, wavelet='db2', strategy='max'):\n",
    "    # images: a list of input images to be fused\n",
    "    # level: the level of wavelet decomposition\n",
    "    # wavelet: the type of wavelet to use\n",
    "    # strategy: the fusion strategy to use (e.g. 'max' or 'mean')\n",
    "\n",
    "    # Perform wavelet decomposition on each input image\n",
    "    coeffs_list = []\n",
    "    for img in images:\n",
    "        coeffs = pywt.wavedec2(img, wavelet, level=level)\n",
    "        coeffs_list.append(coeffs)\n",
    "\n",
    "    # Calculate the sparse coefficients for each image\n",
    "    sparse_list = []\n",
    "    for coeffs in coeffs_list:\n",
    "        sparse_coeffs = []\n",
    "        for coeff in coeffs:\n",
    "            coeff_flat = coeff.flatten()\n",
    "            sparse_coeff = pywt.threshold(coeff_flat, np.std(coeff_flat) / 2)\n",
    "            sparse_coeffs.append(sparse_coeff)\n",
    "        sparse_list.append(sparse_coeffs)\n",
    "\n",
    "    # Fuse the sparse coefficients using the specified strategy\n",
    "    fused_sparse_coeffs = []\n",
    "    for i in range(len(sparse_list[0])):\n",
    "        coeff_i = []\n",
    "        for j in range(len(sparse_list)):\n",
    "            coeff_ij = sparse_list[j][i]\n",
    "            coeff_i.append(coeff_ij)\n",
    "        fused_coeff_i = np.zeros_like(coeff_i[0])\n",
    "        if strategy == 'max':\n",
    "            fused_coeff_i = np.max(coeff_i, axis=0)\n",
    "        elif strategy == 'mean':\n",
    "            fused_coeff_i = np.mean(coeff_i, axis=0)\n",
    "        fused_sparse_coeffs.append(fused_coeff_i)\n",
    "\n",
    "    # Reconstruct the fused image using the fused sparse coefficients\n",
    "    fused_coeffs = []\n",
    "    for coeffs in coeffs_list:\n",
    "        fused_coeffs_i = []\n",
    "        for i in range(len(coeffs)):\n",
    "            coeff_i = coeffs[i]\n",
    "            sparse_coeff_i = sparse_list[0][i]  # Use the sparse coefficients of the first image\n",
    "            fused_sparse_coeff_i = fused_sparse_coeffs[i]\n",
    "            fused_coeff_i = np.zeros_like(coeff_i)\n",
    "            fused_coeff_i[np.abs(coeff_i) >= np.abs(sparse_coeff_i)] = coeff_i[np.abs(coeff_i) >= np.abs(sparse_coeff_i)]\n",
    "            fused_coeff_i[np.abs(coeff_i) < np.abs(sparse_coeff_i)] = fused_sparse_coeff_i[np.abs(coeff_i) < np.abs(sparse_coeff_i)]\n",
    "            fused_coeffs_i.append(fused_coeff_i)\n",
    "        fused_coeffs.append(tuple(fused_coeffs_i))\n",
    "    fused_img = pywt.waverec2(fused_coeffs, wavelet)\n",
    "\n",
    "    return fused_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load two input images\n",
    "img1 = cv2.imread('image1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('image2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Resize the images to the same size\n",
    "img1 = cv2.resize(img1, (256, 256))\n",
    "img2 = cv2.resize(img2, (256, 256))\n",
    "\n",
    "# Perform SRF image fusion\n",
    "fused_img = srf_fusion([img1, img2], level=3, wavelet='db2', strategy='max')\n",
    "\n",
    "# Display the fused image\n",
    "cv2.imshow('Fused Image', fused_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load two input images\n",
    "img1 = cv2.imread('image1.jpg')\n",
    "img2 = cv2.imread('image2.jpg')\n",
    "\n",
    "# Resize the images to the same size\n",
    "img1 = cv2.resize(img1, (256, 256))\n",
    "img2 = cv2.resize(img2, (256, 256))\n",
    "\n",
    "# Generate a weight map based on image contrast\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "blur1 = cv2.GaussianBlur(gray1, (5, 5), 0)\n",
    "blur2 = cv2.GaussianBlur(gray2, (5, 5), 0)\n",
    "diff = cv2.absdiff(blur1, blur2)\n",
    "thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "weight_map = cv2.merge([thresh, thresh, thresh])\n",
    "\n",
    "# Perform Laplacian Pyramid Fusion\n",
    "num_levels = 4\n",
    "l_pyr1 = cv2.pyrDown(img1)\n",
    "l_pyr2 = cv2.pyrDown(img2)\n",
    "gp1 = [img1]\n",
    "gp2 = [img2]\n",
    "lp1 = [l_pyr1]\n",
    "lp2 = [l_pyr2]\n",
    "for i in range(num_levels - 1):\n",
    "    l_pyr1 = cv2.pyrDown(l_pyr1)\n",
    "    l_pyr2 = cv2.pyrDown(l_pyr2)\n",
    "    lp1.append(l_pyr1)\n",
    "    lp2.append(l_pyr2)\n",
    "    g_pyr1 = cv2.subtract(gp1[i], cv2.pyrUp(l_pyr1))\n",
    "    g_pyr2 = cv2.subtract(gp2[i], cv2.pyrUp(l_pyr2))\n",
    "    gp1.append(g_pyr1)\n",
    "    gp2.append(g_pyr2)\n",
    "fused_lp = []\n",
    "for i in range(num_levels):\n",
    "    fused_coeff = cv2.addWeighted(lp1[i], weight_map[i], lp2[i], 1 - weight_map[i], 0)\n",
    "    fused_lp.append(fused_coeff)\n",
    "fused_img = fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = nn.MultiheadAttention(16, 4, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(testing_tensor.reshape(5, 16, -1),testing_tensor.reshape(5, 16, -1), testing_tensor.reshape(5, 16, -1))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape(): argument 'shape' must be tuple of SymInts, but found element of type NoneType at pos 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testing_tensor\u001b[39m.\u001b[39;49mreshape(\u001b[39m5\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mTypeError\u001b[0m: reshape(): argument 'shape' must be tuple of SymInts, but found element of type NoneType at pos 3"
     ]
    }
   ],
   "source": [
    "testing_tensor.reshape(5, 16, None, None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 16, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5, 16))[:,:,None,None].repeat(1, 1, 16, 16).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5, 16))[:,:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5, 16))[:,:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.LayerNorm(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(torch.ones((5, 16))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's implement other fusors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
